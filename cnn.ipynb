{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras import layers, models\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Initial Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=(30, 30, 3)), \n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(106, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Accuracy: 96.89%\n",
    "\n",
    "Test Accuracy: 89.33%\n",
    "\n",
    "The accuracy scores indicate that the initial model is overfitting to the training data. The testing accuracy is also much lower than benchmark models for the individual datasets. Additionally, the filter size of 5 x 5 is too large and struggles to detect smaller and more intricate features. Due to our dataset being extremely large and diverse with a variety of samples, it is evident that a well-performing CNN model must be more complex and have extra non-linearity. This model is too simple, and thus unable to learn accurately on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes\n",
    "\n",
    "To account for the above limitations, we made several changes to our initial model as outlined below.\n",
    "\n",
    "1) Added convolutional layers to the model architecture. This helped us to increase the complexity of the model and adapt to our more complex data.\n",
    "\n",
    "2) Used a smaller filter size of 3 x 3. This ensured that the model was able to detect intricate features while also reducing the number of parameters, thus increasing efficiency.\n",
    "\n",
    "3) Added a dropout layer. A dropout layer was added to reduce overfitting and help the model generalise better. By adding a dropout layer, the model is less likely to be dependent on a certain set of features or parameters.\n",
    "\n",
    "4) Added a third fully connected layer. This allowed us to accomodate for combinations of complex features, and hence include extra non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final CNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 30, 30, 64)        1792      \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 30, 30, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 15, 15, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 15, 15, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 15, 15, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 7, 7, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               3211776   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 106)               27242     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3630506 (13.85 MB)\n",
      "Trainable params: 3630506 (13.85 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNN_model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=(30, 30, 3)),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    \n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),  # Adding dropout for regularization\n",
    "    \n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(106, activation='softmax')\n",
    "])\n",
    "CNN_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
