{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A comparative analysis of two CNN architectures for the purpose of Cross-Country and Multilingual Traffic Sign Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert INTRODUCTION stuff here ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation & Literature Review\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert MOTIVATION stuff here ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert PROBLEM STATEMENT stuff here ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German Traffic Sign Recognition Benchmark (GTSRB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert GTSRB stuff here ...]\n",
    "- Include brief introduction to dataset\n",
    "- Include link to dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinese Traffic Signs Dataset (CTSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert CTSD stuff here ...]\n",
    "- Include brief introduction to dataset\n",
    "- Include link to dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indian Traffic Signs Dataset (ITSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert ITSD stuff here ...]\n",
    "- Include brief introduction to dataset\n",
    "- Include link to dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing, Exploration & Merging\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will load and organise the data from all three datasets, explore the attributes and characteristic of this data, and merge them together to create our final merged dataset. We will also take preprocessing steps like data augmentation to increase diversity, and also enlarge the size of smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras import layers, models\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German Traffic Sign Recognition Benchmark Dataset (GTSRB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the width, height and classses of 5 images from the Train data, and 5 images from the Test data. Note: GTSRB is already split into train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train.csv\n",
    "train_csv_path = os.getcwd() + \"/german/Train.csv\"\n",
    "train_df = pd.read_csv(train_csv_path, usecols=['ClassId', 'Path', 'Width', 'Height'])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test.csv\n",
    "test_csv_path = os.getcwd() + \"/german/Test.csv\"\n",
    "test_df = pd.read_csv(test_csv_path, usecols=['ClassId', 'Path', 'Width', 'Height'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the class mapping from class ID's to actual classes. Please note that this Label Overview is from Kaggle (Link: https://www.kaggle.com/code/shivank856/gtsrb-cnn-98-test-accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTSRB Label Overview (from Kaggle)\n",
    "GTSRB_classes = { 0:'Speed limit (20km/h)',\n",
    "            1:'Speed limit (30km/h)', \n",
    "            2:'Speed limit (50km/h)', \n",
    "            3:'Speed limit (60km/h)', \n",
    "            4:'Speed limit (70km/h)', \n",
    "            5:'Speed limit (80km/h)', \n",
    "            6:'End of speed limit (80km/h)', \n",
    "            7:'Speed limit (100km/h)', \n",
    "            8:'Speed limit (120km/h)', \n",
    "            9:'No passing', \n",
    "            10:'No passing veh over 3.5 tons', \n",
    "            11:'Right-of-way at intersection', \n",
    "            12:'Priority road', \n",
    "            13:'Yield', \n",
    "            14:'Stop', \n",
    "            15:'No vehicles', \n",
    "            16:'Veh > 3.5 tons prohibited', \n",
    "            17:'No entry', \n",
    "            18:'General caution', \n",
    "            19:'Dangerous curve left', \n",
    "            20:'Dangerous curve right', \n",
    "            21:'Double curve', \n",
    "            22:'Bumpy road', \n",
    "            23:'Slippery road', \n",
    "            24:'Road narrows on the right', \n",
    "            25:'Road work', \n",
    "            26:'Traffic signals', \n",
    "            27:'Pedestrians', \n",
    "            28:'Children crossing', \n",
    "            29:'Bicycles crossing', \n",
    "            30:'Beware of ice/snow',\n",
    "            31:'Wild animals crossing', \n",
    "            32:'End speed + passing limits', \n",
    "            33:'Turn right ahead', \n",
    "            34:'Turn left ahead', \n",
    "            35:'Ahead only', \n",
    "            36:'Go straight or right', \n",
    "            37:'Go straight or left', \n",
    "            38:'Keep right', \n",
    "            39:'Keep left', \n",
    "            40:'Roundabout mandatory', \n",
    "            41:'End of no passing', \n",
    "            42:'End no passing veh > 3.5 tons' }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Test and Train data, and resizing all images to 30 x 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_train_data = [] # X_train\n",
    "german_train_labels = [] # X_test\n",
    "classes = 43\n",
    "train_path = os.getcwd() + \"/german/Train\"\n",
    "\n",
    "for i in os.listdir(train_path):\n",
    "    dir = train_path + '/' + i\n",
    "    if os.path.isdir(dir):\n",
    "        for j in os.listdir(dir):\n",
    "            try:\n",
    "                img_path = dir+ '/' +j\n",
    "                img = cv2.imread(img_path,-1)\n",
    "                # resize all images to 30,30\n",
    "                img = cv2.resize(img, (30,30), interpolation = cv2.INTER_NEAREST)\n",
    "                german_train_data.append(img)\n",
    "                german_train_labels.append(i)\n",
    "            except Exception as e:\n",
    "                print(e)    \n",
    "german_train_data = np.array(german_train_data)\n",
    "german_train_labels = np.array(german_train_labels)\n",
    "print(german_train_data.shape, german_train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_test_data = [] # y_train\n",
    "german_test_labels = [] # y_test\n",
    "test_path = os.getcwd() + \"/german/Test\"\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    img_path = os.getcwd() + \"/german/\" + row['Path']\n",
    "    img = cv2.imread(img_path, -1)\n",
    "    img = cv2.resize(img, (30,30), interpolation=cv2.INTER_NEAREST)\n",
    "    german_test_data.append(img)\n",
    "    german_test_labels.append(row['ClassId'])\n",
    "\n",
    "german_test_data = np.array(german_test_data)\n",
    "german_test_labels = np.array(german_test_labels)\n",
    "print(german_test_data.shape, german_test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising 25 images from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices = random.sample(range(len(german_train_data)), 25)\n",
    "# randomly plot 25 images from german_train_data[]\n",
    "fig, axs = plt.subplots(5, 5, figsize=(10, 10))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        index = random_indices[i * 5 + j]\n",
    "        # Display the image\n",
    "        axs[i, j].imshow(german_train_data[index])\n",
    "        axs[i, j].axis('off')\n",
    "        # Display the corresponding label\n",
    "        axs[i, j].set_title(f\"Label: {german_train_labels[index]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinese Traffic Signs Dataset (CTSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_china_dataset():\n",
    "    base_dir = os.getcwd()\n",
    "    csv_path = os.path.join(base_dir, 'china/annotations.csv')\n",
    "    print(\"CSV Path:\", csv_path)\n",
    "\n",
    "\n",
    "    data = pd.read_csv(csv_path)\n",
    "    image_fnames = data['file_name']\n",
    "    categories = data['category']\n",
    "\n",
    "    def load_traffic_sign_image(fname):\n",
    "        img = cv2.imread(fname, -1)\n",
    "        img = cv2.resize(img, (30, 30), interpolation=cv2.INTER_NEAREST)\n",
    "        return img\n",
    "\n",
    "    images = np.array([load_traffic_sign_image(os.path.join(base_dir, 'china/images', fn)) for fn in image_fnames])\n",
    "\n",
    "    return images, categories\n",
    "\n",
    "china_images, china_categories = load_china_dataset()\n",
    "print(china_images.shape, china_categories.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmenting the Data to enlarge size of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotates each image by -10 degrees (left) and 10 degrees (right) and then concatenates \n",
    "# the original and augmented datasets along with their corresponding labels. \n",
    "def rotate_augmentation(images, labels):\n",
    "    augmented_data = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for img, label in zip(images, labels):\n",
    "        # Rotate left\n",
    "        rotated_left = rotate_image(img, angle=-10)\n",
    "        augmented_data.append(rotated_left)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "        # Rotate right\n",
    "        rotated_right = rotate_image(img, angle=10)\n",
    "        augmented_data.append(rotated_right)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "    augmented_data = np.concatenate([images, augmented_data])\n",
    "    augmented_labels = np.concatenate([labels, augmented_labels])\n",
    "\n",
    "    return augmented_data, augmented_labels\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "    center = tuple(np.array(image.shape[1::-1]) / 2)\n",
    "    rot_mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated_image = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_NEAREST)\n",
    "    return rotated_image\n",
    "\n",
    "china_images, china_categories= rotate_augmentation(china_images, china_categories)\n",
    "print(china_images.shape, china_categories.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies random zooming (by 0.15) or width shifting (by 0.1) or height shifting (by 0.1) to each image and then concatenates \n",
    "# the original and augmented datasets along with their corresponding labels. \n",
    "def random_zoom_or_shift(images, labels):\n",
    "    augmented_data = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for img, label in zip(images, labels):\n",
    "        # Randomly choose augmentations types\n",
    "        num_augmentations = np.random.randint(1, 4)  # Choose between 1 to 3 augmentations\n",
    "        augmentation_types = np.random.choice(['zoom', 'width_shift', 'height_shift'], size=num_augmentations, replace=True)\n",
    "        augmented_img = img.copy()\n",
    "        for augmentation_type in augmentation_types:\n",
    "            if augmentation_type == 'zoom':\n",
    "                zoom_factor = np.random.uniform(1 - 0.15, 1 + 0.15)\n",
    "                augmented_img = zoom_image(augmented_img, zoom_factor)\n",
    "            elif augmentation_type == 'width_shift':\n",
    "                width_shift_factor = np.random.uniform(-0.1, 0.1)\n",
    "                augmented_img = shift_width(augmented_img, width_shift_factor)\n",
    "            elif augmentation_type == 'height_shift':\n",
    "                height_shift_factor = np.random.uniform(-0.1, 0.1)\n",
    "                augmented_img = shift_height(augmented_img, height_shift_factor)\n",
    "        augmented_img = cv2.resize(augmented_img, (30, 30), interpolation=cv2.INTER_NEAREST)\n",
    "        augmented_data.append(augmented_img)\n",
    "        augmented_labels.append(label)       \n",
    "\n",
    "    augmented_data = np.concatenate([images, augmented_data])\n",
    "    augmented_labels = np.concatenate([labels, augmented_labels])\n",
    "\n",
    "    return augmented_data, augmented_labels\n",
    "\n",
    "def zoom_image(image, factor):\n",
    "    height, width = image.shape[:2]\n",
    "    new_height, new_width = int(height * factor), int(width * factor)\n",
    "    zoomed_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_NEAREST)\n",
    "    return zoomed_image\n",
    "\n",
    "def shift_width(image, factor):\n",
    "    height, width = image.shape[:2]\n",
    "    shift_pixels = int(width * factor)\n",
    "    shifted_image = np.roll(image, shift_pixels, axis=1)\n",
    "    return shifted_image\n",
    "\n",
    "def shift_height(image, factor):\n",
    "    height, width = image.shape[:2]\n",
    "    shift_pixels = int(height * factor)\n",
    "    shifted_image = np.roll(image, shift_pixels, axis=0)\n",
    "    return shifted_image\n",
    "\n",
    "\n",
    "china_images, china_categories = random_zoom_or_shift(china_images, china_categories)\n",
    "print(china_images.shape, china_categories.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into test and train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china_train_data, china_test_data, china_train_label, china_test_label = train_test_split(china_images, china_categories, test_size=0.25, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(\"china_train_data shape:\", china_train_data.shape)\n",
    "print(\"china_test_data shape:\", china_test_data.shape)\n",
    "print(\"china_train_label shape:\", china_train_label.shape)\n",
    "print(\"china_test_label shape:\", china_test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising 58 imagges from CTSD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classes_with_images(images, categories, num_classes=5, images_per_row=7):\n",
    "    unique_categories = np.unique(categories)\n",
    "    num_rows = -(-num_classes // images_per_row)  # Ceil division to calculate the number of rows\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, images_per_row, figsize=(20, num_rows * 4))\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        for j in range(images_per_row):\n",
    "            index = i * images_per_row + j\n",
    "            if index < num_classes:\n",
    "                class_category = unique_categories[index]\n",
    "                class_images = images[categories == class_category]\n",
    "\n",
    "                # Randomly select an image from the class\n",
    "                random_image = random.choice(class_images)\n",
    "\n",
    "                axes[i, j].imshow(random_image)\n",
    "                axes[i, j].set_title(f'Class {class_category}')\n",
    "                axes[i, j].axis('off')\n",
    "            else:\n",
    "                axes[i, j].axis('off')  # Hide empty subplots\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_classes_with_images(china_images, china_categories, 58)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually labelling classes in the CTSD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelled based on information from: https://en.wikipedia.org/wiki/Road_signs_in_China\n",
    "china_classes = { 0:'Speed limit (5km/h)',\n",
    "                1:'Speed limit (15km/h)', \n",
    "                2:'Speed limit (30km/h)', \n",
    "                3:'Speed limit (40km/h)', \n",
    "                4:'Speed limit (50km/h)', \n",
    "                5:'Speed limit (60km/h)', \n",
    "                6:'Speed limit (70km/h)', \n",
    "                7:'Speed limit (80km/h)', \n",
    "                8:'No proceed straight and left turns', \n",
    "                9:'No proceed straight and right turns', \n",
    "                10:'No proceed straight', \n",
    "                11:'No left turn', \n",
    "                12:'No left and right turns', \n",
    "                13:'No right turn', \n",
    "                14:'No overtaking', \n",
    "                15:'No U-turns', \n",
    "                16:'No motor vehicles', \n",
    "                17:'No honking', \n",
    "                18:'End of speed limit (40km/h)', \n",
    "                19:'End of speed limit (50km/h)', \n",
    "                20:'Go straight or right', \n",
    "                21:'Ahead only', \n",
    "                22:'Turn left ahead', \n",
    "                23:'Go left or right', \n",
    "                24:'Turn right ahead', \n",
    "                25:'Keep left', \n",
    "                26:'Keep right', \n",
    "                27:'Roundabout mandatory', \n",
    "                28:'Lane for automobile', \n",
    "                29:'Honking', \n",
    "                30:'Bicycles only',\n",
    "                31:'U-turn', \n",
    "                32:'Turn left and/or right to detour', \n",
    "                33:'Traffic signals', \n",
    "                34:'General caution', \n",
    "                35:'Pedestrian crossing ahead', \n",
    "                36:'Bicycles crossing', \n",
    "                37:'Children crossing', \n",
    "                38:'Dangerous curve right', \n",
    "                39:'Dangerous curve left', \n",
    "                40:'Steep descent', \n",
    "                41:'Steep ascent', \n",
    "                42:'Tunnel ahead', # not sure\n",
    "                43:'Side road junction ahead on the right',\n",
    "                44:'Side road junction ahead on the left',\n",
    "                45:'Cross-village road',\n",
    "                46:'Double curve, with turn right first, then left', # GTSRB has a general version double curve, should i merge them?\n",
    "                47:'Railroad crossing ahead (without safety barriers)',\n",
    "                48:'Road work',\n",
    "                49:'Multiple curves',\n",
    "                50:'Railroad head (with safety barriers)',\n",
    "                51:'Accident area',\n",
    "                52:'Stop',\n",
    "                53:'No vehicles',\n",
    "                54:'No stopping',\n",
    "                55:'No entry',\n",
    "                56:'Yield',\n",
    "                57:'Control' }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indian Traffic Signs Dataset (ITSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the ITSD Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_india_dataset():\n",
    "    train_data = []  # X_train\n",
    "    train_labels = []  # X_test\n",
    "    classes = 46  # Including 0 to 45\n",
    "    dataset_path = os.path.join(os.getcwd(), \"india\", \"Images\")\n",
    "\n",
    "    for label in range(classes):\n",
    "        label_path = os.path.join(dataset_path, str(label))\n",
    "        if os.path.isdir(label_path):\n",
    "            for img_name in os.listdir(label_path):\n",
    "                try:\n",
    "                    img_path = os.path.join(label_path, img_name)\n",
    "                    img = cv2.imread(img_path)\n",
    "                    \n",
    "                    # Convert to RGB if the image has a single channel (grayscale)\n",
    "                    if img.shape[-1] == 1:\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "                    \n",
    "                    # Resize all images to 30x30\n",
    "                    img = cv2.resize(img, (30, 30), interpolation=cv2.INTER_NEAREST)\n",
    "                    \n",
    "                    train_data.append(img)\n",
    "                    train_labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "    train_data = np.array(train_data)\n",
    "    train_labels = np.array(train_labels)\n",
    "    return train_data, train_labels\n",
    "    \n",
    "India_train, India_label = load_india_dataset()\n",
    "print(India_train.shape, India_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same data augmentation functions outlined above (in CTSD section) to enlarge the size of the ITSD training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase ths size of the dataset by using data augmentation again like the chinese dataset\n",
    "India_train, India_label= rotate_augmentation(India_train, India_label)\n",
    "India_train, India_label = random_zoom_or_shift(India_train, India_label)\n",
    "print(India_train.shape, India_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into Train and Test datasets, and then showing the sizes of images, as well as number of images and labels in this split data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "india_train_data, india_test_data, india_train_label, india_test_label = train_test_split(India_train, India_label, test_size=0.25, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(\"india_train_data shape:\", india_train_data.shape)\n",
    "print(\"india_test_data shape:\", india_test_data.shape)\n",
    "print(\"india_train_label shape:\", india_train_label.shape)\n",
    "print(\"india_test_label shape:\", india_test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually labelling classes in ITSD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually label \n",
    "india_classes = {\n",
    "      0:'Yield',\n",
    "      1:'No proceed straight',\n",
    "      2:'One-way traffic',\n",
    "      3:'One-way traffic',\n",
    "      4:'No vehicles in both directions',\n",
    "      5:'No entry for cycles',\n",
    "      6:'No entry for goods vehicles',\n",
    "      7:'No entry for pedestrians',\n",
    "      8:'No entry for bullock carts',\n",
    "      9:'No entry for hand carts',\n",
    "      10:'No motor vehicles',\n",
    "      11:'Height limit',\n",
    "      12:'Weight limit',\n",
    "      13:'Axle weight limit',\n",
    "      14:'Length limit',\n",
    "      15:'No left turn',\n",
    "      16:'No right turn',\n",
    "      17:'No overtaking',\n",
    "      18:'Speed limit (90 km/h)',\n",
    "      19:'Speed limit (110 km/h)',\n",
    "      20:'No honking',\n",
    "      21:'No parking',\n",
    "      22:'No stopping',\n",
    "      23:'Turn left ahead',\n",
    "      24:'Turn right ahead',\n",
    "      25:'Steep descent',\n",
    "      26:'Steep ascent',\n",
    "      27:'Road narrows on the right',\n",
    "      28:'Narrow bridge',\n",
    "      29:'Unprotected quay',\n",
    "      30:'Road hump',\n",
    "      31:'Bumpy road',\n",
    "      32:'Loose gravel',\n",
    "      33:'Falling rocks',\n",
    "      34:'Wild animals crossing',\n",
    "      35:'Crossroads',\n",
    "      36:'Side road junction',\n",
    "      37:'Oblique side road junction',\n",
    "      38:'T-junction',\n",
    "      39:'Y-junction',\n",
    "      40:'Staggered side road junction',\n",
    "      41:'Roundabout mandatory',\n",
    "      42:'Railroad head (with safety barriers)',\n",
    "      43:'Unguarded level crossing ahead',\n",
    "      44:'Parking',\n",
    "      45:'Bus stop'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging GTRSB and CTSD together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to merge two classes dict together\n",
    "def merge_classes(classes1, classes2):\n",
    "    merged_classes = {}\n",
    "    used_values = set()\n",
    "\n",
    "    for key1, value1 in classes1.items():\n",
    "        if value1.lower() not in used_values:\n",
    "            merged_classes[key1] = value1\n",
    "            used_values.add(value1.lower())\n",
    "\n",
    "    for key2, value2 in classes2.items():\n",
    "        if value2.lower() not in used_values:\n",
    "            merged_classes[max(merged_classes.keys()) + 1] = value2\n",
    "            used_values.add(value2.lower())\n",
    "\n",
    "    return merged_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to merge two dataset together \n",
    "def merge_datasets(dataset1_data, dataset1_labels, dataset1_classes, dataset2_data, dataset2_labels, dataset2_classes):\n",
    "    merged_data = []\n",
    "    merged_labels = []\n",
    "    \n",
    "    class_mapping = merge_classes(dataset1_classes, dataset2_classes)\n",
    "    \n",
    "    def find_key_by_value(dict, target_value):\n",
    "        for key, value in dict.items():\n",
    "            if value.lower() == target_value.lower():\n",
    "                return key\n",
    "\n",
    "    # Merge the datasets based on class mapping\n",
    "    for data, label in zip(dataset1_data, dataset1_labels):\n",
    "        value = dataset1_classes.get(int(label))\n",
    "        new_key = find_key_by_value(class_mapping, value)\n",
    "        merged_labels.append(new_key)\n",
    "        merged_data.append(data)\n",
    "\n",
    "    for data, label in zip(dataset2_data, dataset2_labels):\n",
    "        value = dataset2_classes.get(int(label))\n",
    "        new_key = find_key_by_value(class_mapping, value)\n",
    "        merged_labels.append(new_key)\n",
    "        merged_data.append(data)\n",
    "\n",
    "    return np.array(merged_data), np.array(merged_labels), class_mapping\n",
    "\n",
    "\n",
    "GTRSB_data = german_train_data\n",
    "GTRSB_labels = german_train_labels\n",
    "china_images = china_train_data\n",
    "china_categories = china_train_label\n",
    "# merge GTRSB and Chinese dataset\n",
    "merged_data, merged_labels, merged_class_mapping = merge_datasets(GTRSB_data, GTRSB_labels, GTSRB_classes, china_images, china_categories, china_classes)\n",
    "print(\"GTRSB Dataset Shapes:\")\n",
    "print(GTRSB_data.shape, GTRSB_labels.shape)\n",
    "print(\"China Dataset Shapes:\")\n",
    "print(china_images.shape, china_categories.shape)\n",
    "# Display the merged dataset shapes\n",
    "print(\"Merged Dataset Shapes:\")\n",
    "print(\"Merged Data Shape:\", merged_data.shape)\n",
    "print(\"Merged Labels Shape:\", merged_labels.shape)\n",
    "\n",
    "# Display the class mapping after merging\n",
    "print(\"Class Mapping After Merging:\")\n",
    "for key, value in merged_class_mapping.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging ITSD datest with the already merged GTSRB + CTSD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data, merged_labels, merged_class_mapping = merge_datasets(merged_data, merged_labels, merged_class_mapping, india_train_data, india_train_label, india_classes)\n",
    "# Display the merged dataset shapes\n",
    "print(\"Merged Dataset Shapes:\")\n",
    "print(\"Merged Data Shape:\", merged_data.shape)\n",
    "print(\"Merged Labels Shape:\", merged_labels.shape)\n",
    "\n",
    "# Display the class mapping after merging\n",
    "print(\"Class Mapping After Merging:\")\n",
    "for key, value in merged_class_mapping.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Merged Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of data in each of the three datasets as well as in the merged dataset, as well as the creation of an overall (merged) testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "print(\"GTRSB Train data and label shape:\")\n",
    "print(GTRSB_data.shape, GTRSB_labels.shape) \n",
    "print(\"China Train data and label shape:\")\n",
    "print(china_train_data.shape, china_train_label.shape)\n",
    "print(\"India Train data and label shape:\")\n",
    "print(india_train_data.shape, india_train_label.shape)\n",
    "print(\"Merged Train shape:\")\n",
    "print(merged_data.shape, merged_labels.shape)\n",
    "print()\n",
    "german_test_labels = np.squeeze(german_test_labels)\n",
    "# Testing set\n",
    "print(\"GTRSB Test data and label shape:\")\n",
    "print(german_test_data.shape, german_test_labels.shape) \n",
    "print(\"China Test data and label shape:\")\n",
    "print(china_test_data.shape, china_test_label.shape)\n",
    "print(\"India Test data and label shape:\")\n",
    "print(india_test_data.shape, india_test_label.shape)\n",
    "# create an overall testset \n",
    "overall_test_data, overall_test_labels, overall_classes = merge_datasets(german_test_data, german_test_labels, GTSRB_classes, china_test_data, china_test_label, china_classes)\n",
    "overall_test_data, overall_test_labels, overall_classes = merge_datasets(overall_test_data, overall_test_labels, overall_classes, india_test_data, india_test_label, india_classes)\n",
    "\n",
    "print(\"Overall Test data and label shape:\")\n",
    "print(overall_test_data.shape, overall_test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalising the Train images in the merged data, as well as Test images in the individual datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data / 255.0 # Train data\n",
    "overall_test_data = overall_test_data / 255.0\n",
    "german_test_data = german_test_data / 255.0 # GTRSB only test set\n",
    "china_test_data = china_test_data / 255.0\n",
    "india_test_data = india_test_data / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Train and Test labels of merged data into one hot encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_labels = to_categorical(merged_labels, 106) # Train labels\n",
    "overall_test_labels = to_categorical(overall_test_labels, 106)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the merged data into training and validation tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets (80%, 20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(merged_data, merged_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of Merged Dataset\n",
    "\n",
    "[Including merged dataset exploration stuff here...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train a Convolutional Neural Network (CNN) model, and a ResNet model on the merged dataset. These two models are defined in cnn.py and resnet.py respectively. To see further information on how these models were created and defined, as well as steps taken to increase accuracy and efficiency, please refer to these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.cnn import CNN_model\n",
    "from ipynb.fs.full.resnet import RESNET_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the Model, and printing a summary of the Layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "CNN_model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "CNN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "CNN_history = CNN_model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "CNN_model.save(\"CNN_traffic_sign_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the Model, and printing a summary of the Layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "RESNET_model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "RESNET_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "RESNET_history = RESNET_model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "RESNET_model.save(\"resnet_traffic_sign_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Accuracy on Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Accuracy Plot derived from: https://github.com/deepak2233/Traffic-Signs-Recognition-using-CNN-Keras/blob/main/Model/Traffic%20Signs%20Recognition%20using%20CNN%20%26%20Keras%20with%2098%25%20Accuracy.ipynb\n",
    "def train_plot(history):\n",
    "    plt.plot(history.history['accuracy'], label='training accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Loss \n",
    "    plt.plot(history.history['loss'], label='training loss')\n",
    "    plt.plot(history.history['val_loss'], label='val loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "train_plot(CNN_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall test using merged test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, num_classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    \n",
    "    classes = [str(i) for i in range(num_classes)]\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(num_classes, num_classes))\n",
    "    sns.set(font_scale=1.2)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, xticklabels=classes, yticklabels=classes)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_true, y_pred, zero_division=1))\n",
    "\n",
    "def analyze_confusion_matrix(y_true, y_pred, num_classes):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Identify high-error pairs\n",
    "    high_error_pairs = []\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            if i != j and cm[i, j] > 10:  # Adjust the threshold as needed\n",
    "                high_error_pairs.append((i, j, cm[i, j]))\n",
    "\n",
    "    # Print high-error pairs\n",
    "    if high_error_pairs:\n",
    "        print(\"High-Error Pairs:\")\n",
    "        for pair in high_error_pairs:\n",
    "            true_class, predicted_class, count = pair\n",
    "            print(f\"True Class {true_class} predicted as {predicted_class}: {count} instances\")\n",
    "    else:\n",
    "        print(\"No high-error pairs found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on merged test set\n",
    "model = models.load_model(\"CNN_traffic_sign_model\")\n",
    "y_pred = model.predict(overall_test_data)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(overall_test_labels, axis=1)\n",
    "print_metrics(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "plot_confusion_matrix(y_true, y_pred, 106, title='Overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_confusion_matrix(y_true, y_pred, 106)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Evaluation on GTSRB Test Data [German Only]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_labels(labels, current_class_mapping, target_class_mapping):\n",
    "    remapped_labels = []\n",
    "\n",
    "    for label in labels:\n",
    "        # print(current_class_mapping)\n",
    "        original_value = current_class_mapping.get(label, None)\n",
    "        if original_value is not None:\n",
    "            original_key = next((key for key, value in target_class_mapping.items() if value.lower() == original_value.lower()), None)\n",
    "            if original_key is not None:\n",
    "                remapped_labels.append(original_key)\n",
    "            else:\n",
    "                remapped_labels.append(-1)\n",
    "        else:\n",
    "            remapped_labels.append(-1)\n",
    "\n",
    "    return remapped_labels\n",
    "\n",
    "def map_output_to_target_classes(y_pred, target_classes_mapping):\n",
    "    # Create a reverse mapping for the target classes\n",
    "    reverse_target_mapping = {v: k for k, v in target_classes_mapping.items()}\n",
    "\n",
    "    # Map the output to target classes\n",
    "    y_pred_target_classes = [reverse_target_mapping[label] for label in y_pred]\n",
    "\n",
    "    return y_pred_target_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remap GTRSB test labels to match the 106 classes label format\n",
    "german_test_labels = remap_labels(german_test_labels, GTSRB_classes, merged_class_mapping)\n",
    "german_test_labels = to_categorical(german_test_labels, 106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on GTRSB only\n",
    "model = models.load_model(\"CNN_traffic_sign_model\")\n",
    "y_pred = model.predict(german_test_data)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(german_test_labels, axis=1)\n",
    "print(len(np.unique(y_true)))\n",
    "print_metrics(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_43 = remap_labels(y_pred, merged_class_mapping, GTSRB_classes)\n",
    "y_true_43 = remap_labels(y_true, merged_class_mapping, GTSRB_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "plot_confusion_matrix(y_true_43, y_pred_43, 44, title='GTRSB only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on CTSD Test Data [China Only]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china_test_label = remap_labels(china_test_label, china_classes, merged_class_mapping)\n",
    "china_test_label = to_categorical(china_test_label, 106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(\"CNN_traffic_sign_model\")\n",
    "y_pred = model.predict(china_test_data)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(china_test_label, axis=1)\n",
    "print(len(np.unique(y_true)))\n",
    "print_metrics(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_58 = remap_labels(y_pred, merged_class_mapping, china_classes)\n",
    "y_true_58 = remap_labels(y_true, merged_class_mapping, china_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "plot_confusion_matrix(y_true_58, y_pred_58, 58, title='China Dataset only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on ITSD Test Data [India Only]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_test_label = remap_labels(india_test_label, india_classes, merged_class_mapping)\n",
    "india_test_label = to_categorical(india_test_label, 106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(\"CNN_traffic_sign_model\")\n",
    "y_pred = model.predict(india_test_data)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(india_test_label, axis=1)\n",
    "print(len(np.unique(y_true)))\n",
    "print_metrics(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_45 = remap_labels(y_pred, merged_class_mapping, india_classes)\n",
    "y_true_45 = remap_labels(y_true, merged_class_mapping, india_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "plot_confusion_matrix(y_true_45, y_pred_45, 45, title='India Dataset only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Accuracy on Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plot(RESNET_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall test using merged test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on merged test set\n",
    "model = models.load_model(\"RESNET_traffic_sign_model\")\n",
    "y_pred = model.predict(overall_test_data)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(overall_test_labels, axis=1)\n",
    "print_metrics(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "plot_confusion_matrix(y_true, y_pred, 106, title='Overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_confusion_matrix(y_true, y_pred, 106)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Evaluation on GTSRB Test Data [German Only]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on GTRSB only\n",
    "model = models.load_model(\"RESNET_traffic_sign_model\")\n",
    "y_pred = model.predict(german_test_data)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(german_test_labels, axis=1)\n",
    "print(len(np.unique(y_true)))\n",
    "print_metrics(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_43 = remap_labels(y_pred, merged_class_mapping, GTSRB_classes)\n",
    "y_true_43 = remap_labels(y_true, merged_class_mapping, GTSRB_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "plot_confusion_matrix(y_true_43, y_pred_43, 44, title='GTRSB only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on CTSD Test Data [China Only]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(\"RESNET_traffic_sign_model\")\n",
    "y_pred = model.predict(china_test_data)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(china_test_label, axis=1)\n",
    "print(len(np.unique(y_true)))\n",
    "print_metrics(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_58 = remap_labels(y_pred, merged_class_mapping, china_classes)\n",
    "y_true_58 = remap_labels(y_true, merged_class_mapping, china_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "plot_confusion_matrix(y_true_58, y_pred_58, 58, title='China Dataset only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on ITSD Test Data [India Only]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(\"RESNET_traffic_sign_model\")\n",
    "y_pred = model.predict(india_test_data)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(india_test_label, axis=1)\n",
    "print(len(np.unique(y_true)))\n",
    "print_metrics(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_45 = remap_labels(y_pred, merged_class_mapping, india_classes)\n",
    "y_true_45 = remap_labels(y_true, merged_class_mapping, india_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "plot_confusion_matrix(y_true_45, y_pred_45, 45, title='India Dataset only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Accuracy\n",
    "\n",
    "[Insert Accuracy Comparison Stuff here ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Efficiency & Performance\n",
    "\n",
    "[Insert Performance Comparison Stuff here ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strengths\n",
    "\n",
    "[Insert Strengths of our Models Stuff here ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weaknesses\n",
    "\n",
    "[Insert Weaknesses of our Models Stuff here ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "\n",
    "[Insert Future Work Stuff here ...]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
