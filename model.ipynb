{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Signs Recognition using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# from keras.models import Sequential, load_model\n",
    "# from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
    "from keras import layers, models\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from china.china_dataset_reader import load_traffic_sign_dataset\n",
    "from india.india_dataset_reader import load_india_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code reference:\n",
    "\n",
    "https://github.com/deepak2233/Traffic-Signs-Recognition-using-CNN-Keras/blob/main/Model/Traffic%20Signs%20Recognition%20using%20CNN%20%26%20Keras%20with%2098%25%20Accuracy.ipynb\n",
    "\n",
    "https://www.kaggle.com/code/osamaabidoo/98-accuracy-on-german-traffic-sign-recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GTSRB Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train.csv\n",
    "train_csv_path = os.getcwd() + \"/german/Train.csv\"\n",
    "train_df = pd.read_csv(train_csv_path, usecols=['ClassId', 'Path', 'Width', 'Height'])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test.csv\n",
    "test_csv_path = os.getcwd() + \"/german/Test.csv\"\n",
    "test_df = pd.read_csv(test_csv_path, usecols=['ClassId', 'Path', 'Width', 'Height'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTSRB Label Overview (from Kaggle)\n",
    "GTSRB_classes = { 0:'Speed limit (20km/h)',\n",
    "            1:'Speed limit (30km/h)', \n",
    "            2:'Speed limit (50km/h)', \n",
    "            3:'Speed limit (60km/h)', \n",
    "            4:'Speed limit (70km/h)', \n",
    "            5:'Speed limit (80km/h)', \n",
    "            6:'End of speed limit (80km/h)', \n",
    "            7:'Speed limit (100km/h)', \n",
    "            8:'Speed limit (120km/h)', \n",
    "            9:'No passing', \n",
    "            10:'No passing veh over 3.5 tons', \n",
    "            11:'Right-of-way at intersection', \n",
    "            12:'Priority road', \n",
    "            13:'Yield', \n",
    "            14:'Stop', \n",
    "            15:'No vehicles', \n",
    "            16:'Veh > 3.5 tons prohibited', \n",
    "            17:'No entry', \n",
    "            18:'General caution', \n",
    "            19:'Dangerous curve left', \n",
    "            20:'Dangerous curve right', \n",
    "            21:'Double curve', \n",
    "            22:'Bumpy road', \n",
    "            23:'Slippery road', \n",
    "            24:'Road narrows on the right', \n",
    "            25:'Road work', \n",
    "            26:'Traffic signals', \n",
    "            27:'Pedestrians', \n",
    "            28:'Children crossing', \n",
    "            29:'Bicycles crossing', \n",
    "            30:'Beware of ice/snow',\n",
    "            31:'Wild animals crossing', \n",
    "            32:'End speed + passing limits', \n",
    "            33:'Turn right ahead', \n",
    "            34:'Turn left ahead', \n",
    "            35:'Ahead only', \n",
    "            36:'Go straight or right', \n",
    "            37:'Go straight or left', \n",
    "            38:'Keep right', \n",
    "            39:'Keep left', \n",
    "            40:'Roundabout mandatory', \n",
    "            41:'End of no passing', \n",
    "            42:'End no passing veh > 3.5 tons' }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the GTSRB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [] # X_train\n",
    "train_labels = [] # X_test\n",
    "classes = 43\n",
    "train_path = os.getcwd() + \"/german/Train\"\n",
    "\n",
    "for i in os.listdir(train_path):\n",
    "    dir = train_path + '/' + i\n",
    "    if os.path.isdir(dir):\n",
    "        for j in os.listdir(dir):\n",
    "            try:\n",
    "                img_path = dir+ '/' +j\n",
    "                img = cv2.imread(img_path,-1)\n",
    "                # resize all images to 30,30\n",
    "                img = cv2.resize(img, (30,30), interpolation = cv2.INTER_NEAREST)\n",
    "                train_data.append(img)\n",
    "                train_labels.append(i)\n",
    "            except Exception as e:\n",
    "                print(e)    \n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "print(train_data.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [] # y_train\n",
    "test_labels = [] # y_test\n",
    "test_path = os.getcwd() + \"/german/Test\"\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    img_path = os.getcwd() + \"/german/\" + row['Path']\n",
    "    img = cv2.imread(img_path, -1)\n",
    "    img = cv2.resize(img, (30,30), interpolation=cv2.INTER_NEAREST)\n",
    "    test_data.append(img)\n",
    "    test_labels.append(row['ClassId'])\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)\n",
    "print(test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Train Data in GTSRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices = random.sample(range(len(train_data)), 25)\n",
    "# randomly plot 25 images from train_data[]\n",
    "fig, axs = plt.subplots(5, 5, figsize=(10, 10))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        index = random_indices[i * 5 + j]\n",
    "        # Display the image\n",
    "        axs[i, j].imshow(train_data[index])\n",
    "        axs[i, j].axis('off')\n",
    "        # Display the corresponding label\n",
    "        axs[i, j].set_title(f\"Label: {train_labels[index]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Chinese Traffic Sign Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china_images, china_categories = load_traffic_sign_dataset()\n",
    "print(china_images.shape, china_categories.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotates each image by -10 degrees (left) and 10 degrees (right) and then concatenates \n",
    "# the original and augmented datasets along with their corresponding labels. \n",
    "def rotate_augmentation(images, labels):\n",
    "    augmented_data = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for img, label in zip(images, labels):\n",
    "        # Rotate left\n",
    "        rotated_left = rotate_image(img, angle=-10)\n",
    "        augmented_data.append(rotated_left)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "        # Rotate right\n",
    "        rotated_right = rotate_image(img, angle=10)\n",
    "        augmented_data.append(rotated_right)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "    augmented_data = np.concatenate([images, augmented_data])\n",
    "    augmented_labels = np.concatenate([labels, augmented_labels])\n",
    "\n",
    "    return augmented_data, augmented_labels\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "    center = tuple(np.array(image.shape[1::-1]) / 2)\n",
    "    rot_mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated_image = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_NEAREST)\n",
    "    return rotated_image\n",
    "\n",
    "china_images, china_categories= rotate_augmentation(china_images, china_categories)\n",
    "print(china_images.shape, china_categories.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies random zooming (by 0.15) or width shifting (by 0.1) or height shifting (by 0.1) to each image and then concatenates \n",
    "# the original and augmented datasets along with their corresponding labels. \n",
    "def random_zoom_or_shift(images, labels):\n",
    "    augmented_data = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for img, label in zip(images, labels):\n",
    "        # Randomly choose augmentations types\n",
    "        num_augmentations = np.random.randint(1, 4)  # Choose between 1 to 3 augmentations\n",
    "        augmentation_types = np.random.choice(['zoom', 'width_shift', 'height_shift'], size=num_augmentations, replace=True)\n",
    "        augmented_img = img.copy()\n",
    "        for augmentation_type in augmentation_types:\n",
    "            if augmentation_type == 'zoom':\n",
    "                zoom_factor = np.random.uniform(1 - 0.15, 1 + 0.15)\n",
    "                augmented_img = zoom_image(augmented_img, zoom_factor)\n",
    "            elif augmentation_type == 'width_shift':\n",
    "                width_shift_factor = np.random.uniform(-0.1, 0.1)\n",
    "                augmented_img = shift_width(augmented_img, width_shift_factor)\n",
    "            elif augmentation_type == 'height_shift':\n",
    "                height_shift_factor = np.random.uniform(-0.1, 0.1)\n",
    "                augmented_img = shift_height(augmented_img, height_shift_factor)\n",
    "        augmented_img = cv2.resize(augmented_img, (30, 30), interpolation=cv2.INTER_NEAREST)\n",
    "        augmented_data.append(augmented_img)\n",
    "        augmented_labels.append(label)       \n",
    "\n",
    "    augmented_data = np.concatenate([images, augmented_data])\n",
    "    augmented_labels = np.concatenate([labels, augmented_labels])\n",
    "\n",
    "    return augmented_data, augmented_labels\n",
    "\n",
    "def zoom_image(image, factor):\n",
    "    height, width = image.shape[:2]\n",
    "    new_height, new_width = int(height * factor), int(width * factor)\n",
    "    zoomed_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_NEAREST)\n",
    "    return zoomed_image\n",
    "\n",
    "def shift_width(image, factor):\n",
    "    height, width = image.shape[:2]\n",
    "    shift_pixels = int(width * factor)\n",
    "    shifted_image = np.roll(image, shift_pixels, axis=1)\n",
    "    return shifted_image\n",
    "\n",
    "def shift_height(image, factor):\n",
    "    height, width = image.shape[:2]\n",
    "    shift_pixels = int(height * factor)\n",
    "    shifted_image = np.roll(image, shift_pixels, axis=0)\n",
    "    return shifted_image\n",
    "\n",
    "\n",
    "china_images, china_categories = random_zoom_or_shift(china_images, china_categories)\n",
    "print(china_images.shape, china_categories.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split China dataset to Train and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china_train_data, china_test_data, china_train_label, china_test_label = train_test_split(china_images, china_categories, test_size=0.25, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(\"china_train_data shape:\", china_train_data.shape)\n",
    "print(\"china_test_data shape:\", china_test_data.shape)\n",
    "print(\"china_train_label shape:\", china_train_label.shape)\n",
    "print(\"china_test_label shape:\", china_test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_classes_with_images(images, categories, num_classes=5, images_per_row=7):\n",
    "#     unique_categories = np.unique(categories)\n",
    "#     num_rows = -(-num_classes // images_per_row)  # Ceil division to calculate the number of rows\n",
    "\n",
    "#     fig, axes = plt.subplots(num_rows, images_per_row, figsize=(20, num_rows * 4))\n",
    "\n",
    "#     for i in range(num_rows):\n",
    "#         for j in range(images_per_row):\n",
    "#             index = i * images_per_row + j\n",
    "#             if index < num_classes:\n",
    "#                 class_category = unique_categories[index]\n",
    "#                 class_images = images[categories == class_category]\n",
    "\n",
    "#                 # Randomly select an image from the class\n",
    "#                 random_image = random.choice(class_images)\n",
    "\n",
    "#                 axes[i, j].imshow(random_image)\n",
    "#                 axes[i, j].set_title(f'Class {class_category}')\n",
    "#                 axes[i, j].axis('off')\n",
    "#             else:\n",
    "#                 axes[i, j].axis('off')  # Hide empty subplots\n",
    "\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_classes_with_images(china_images, china_categories, 58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_classes_with_images(train_data, train_labels, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually label the classes in chinese dataset\n",
    "# labelled based on the name on https://en.wikipedia.org/wiki/Road_signs_in_China\n",
    "china_classes = { 0:'Speed limit (5km/h)',\n",
    "                1:'Speed limit (15km/h)', \n",
    "                2:'Speed limit (30km/h)', \n",
    "                3:'Speed limit (40km/h)', \n",
    "                4:'Speed limit (50km/h)', \n",
    "                5:'Speed limit (60km/h)', \n",
    "                6:'Speed limit (70km/h)', \n",
    "                7:'Speed limit (80km/h)', \n",
    "                8:'No proceed straight and left turns', \n",
    "                9:'No proceed straight and right turns', \n",
    "                10:'No proceed straight', \n",
    "                11:'No left turn', \n",
    "                12:'No left and right turns', \n",
    "                13:'No right turn', \n",
    "                14:'No overtaking', \n",
    "                15:'No U-turns', \n",
    "                16:'No motor vehicles', \n",
    "                17:'No honking', \n",
    "                18:'End of speed limit (40km/h)', \n",
    "                19:'End of speed limit (50km/h)', \n",
    "                20:'Go straight or right', \n",
    "                21:'Ahead only', \n",
    "                22:'Turn left ahead', \n",
    "                23:'Go left or right', \n",
    "                24:'Turn right ahead', \n",
    "                25:'Keep left', \n",
    "                26:'Keep right', \n",
    "                27:'Roundabout mandatory', \n",
    "                28:'Lane for automobile', \n",
    "                29:'Honking', \n",
    "                30:'Bicycles only',\n",
    "                31:'U-turn', \n",
    "                32:'Turn left and/or right to detour', \n",
    "                33:'Traffic signals', \n",
    "                34:'General caution', \n",
    "                35:'Pedestrian crossing ahead', \n",
    "                36:'Bicycles crossing', \n",
    "                37:'Children crossing', \n",
    "                38:'Dangerous curve right', \n",
    "                39:'Dangerous curve left', \n",
    "                40:'Steep descent', \n",
    "                41:'Steep ascent', \n",
    "                42:'Tunnel ahead', # not sure\n",
    "                43:'Side road junction ahead on the right',\n",
    "                44:'Side road junction ahead on the left',\n",
    "                45:'Cross-village road',\n",
    "                46:'Double curve, with turn right first, then left', # GTSRB has a general version double curve, should i merge them?\n",
    "                47:'Railroad crossing ahead (without safety barriers)',\n",
    "                48:'Road work',\n",
    "                49:'Multiple curves',\n",
    "                50:'Railroad head (with safety barriers)',\n",
    "                51:'Accident area',\n",
    "                52:'Stop',\n",
    "                53:'No vehicles',\n",
    "                54:'No stopping',\n",
    "                55:'No entry',\n",
    "                56:'Yield',\n",
    "                57:'Control' }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge GTRSB and Chinese dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to merge two classes dict together\n",
    "def merge_classes(classes1, classes2):\n",
    "    merged_classes = {}\n",
    "    used_values = set()\n",
    "\n",
    "    for key1, value1 in classes1.items():\n",
    "        if value1.lower() not in used_values:\n",
    "            merged_classes[key1] = value1\n",
    "            used_values.add(value1.lower())\n",
    "\n",
    "    for key2, value2 in classes2.items():\n",
    "        if value2.lower() not in used_values:\n",
    "            merged_classes[max(merged_classes.keys()) + 1] = value2\n",
    "            used_values.add(value2.lower())\n",
    "\n",
    "    return merged_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to merge two dataset together \n",
    "def merge_datasets(dataset1_data, dataset1_labels, dataset1_classes, dataset2_data, dataset2_labels, dataset2_classes):\n",
    "    merged_data = []\n",
    "    merged_labels = []\n",
    "    \n",
    "    class_mapping = merge_classes(dataset1_classes, dataset2_classes)\n",
    "    \n",
    "    def find_key_by_value(dict, target_value):\n",
    "        for key, value in dict.items():\n",
    "            if value.lower() == target_value.lower():\n",
    "                return key\n",
    "\n",
    "    # Merge the datasets based on class mapping\n",
    "    for data, label in zip(dataset1_data, dataset1_labels):\n",
    "        value = dataset1_classes.get(int(label))\n",
    "        new_key = find_key_by_value(class_mapping, value)\n",
    "        merged_labels.append(new_key)\n",
    "        merged_data.append(data)\n",
    "\n",
    "    for data, label in zip(dataset2_data, dataset2_labels):\n",
    "        value = dataset2_classes.get(int(label))\n",
    "        new_key = find_key_by_value(class_mapping, value)\n",
    "        merged_labels.append(new_key)\n",
    "        merged_data.append(data)\n",
    "\n",
    "    return np.array(merged_data), np.array(merged_labels), class_mapping\n",
    "\n",
    "\n",
    "# merge GTRSB train and test data back together\n",
    "# GTRSB_data = np.concatenate([train_data, test_data], axis=0)\n",
    "# GTRSB_labels = np.concatenate([train_labels, test_labels], axis=0)\n",
    "GTRSB_data = train_data\n",
    "GTRSB_labels = train_labels\n",
    "china_images = china_train_data\n",
    "china_categories = china_train_label\n",
    "# merge GTRSB and Chinese dataset\n",
    "merged_data, merged_labels, merged_class_mapping = merge_datasets(GTRSB_data, GTRSB_labels, GTSRB_classes, china_images, china_categories, china_classes)\n",
    "print(\"GTRSB Dataset Shapes:\")\n",
    "print(GTRSB_data.shape, GTRSB_labels.shape)\n",
    "print(\"China Dataset Shapes:\")\n",
    "print(china_images.shape, china_categories.shape)\n",
    "# Display the merged dataset shapes\n",
    "print(\"Merged Dataset Shapes:\")\n",
    "print(\"Merged Data Shape:\", merged_data.shape)\n",
    "print(\"Merged Labels Shape:\", merged_labels.shape)\n",
    "\n",
    "# Display the class mapping after merging\n",
    "print(\"Class Mapping After Merging:\")\n",
    "for key, value in merged_class_mapping.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_classes_with_images(merged_data, merged_labels, 78)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the India Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "India_train, India_label = load_india_dataset()\n",
    "print(India_train.shape, India_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase ths size of the dataset by using data augmentation again like the chinese dataset\n",
    "India_train, India_label= rotate_augmentation(India_train, India_label)\n",
    "India_train, India_label = random_zoom_or_shift(India_train, India_label)\n",
    "print(India_train.shape, India_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "india_train_data, india_test_data, india_train_label, india_test_label = train_test_split(India_train, India_label, test_size=0.25, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(\"india_train_data shape:\", india_train_data.shape)\n",
    "print(\"india_test_data shape:\", india_test_data.shape)\n",
    "print(\"india_train_label shape:\", india_train_label.shape)\n",
    "print(\"india_test_label shape:\", india_test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually label \n",
    "india_classes = {\n",
    "      0:'Yield',\n",
    "      1:'No proceed straight',\n",
    "      2:'One-way traffic',\n",
    "      3:'One-way traffic',\n",
    "      4:'No vehicles in both directions',\n",
    "      5:'No entry for cycles',\n",
    "      6:'No entry for goods vehicles',\n",
    "      7:'No entry for pedestrians',\n",
    "      8:'No entry for bullock carts',\n",
    "      9:'No entry for hand carts',\n",
    "      10:'No motor vehicles',\n",
    "      11:'Height limit',\n",
    "      12:'Weight limit',\n",
    "      13:'Axle weight limit',\n",
    "      14:'Length limit',\n",
    "      15:'No left turn',\n",
    "      16:'No right turn',\n",
    "      17:'No overtaking',\n",
    "      18:'Speed limit (90 km/h)',\n",
    "      19:'Speed limit (110 km/h)',\n",
    "      20:'No honking',\n",
    "      21:'No parking',\n",
    "      22:'No stopping',\n",
    "      23:'Turn left ahead',\n",
    "      24:'Turn right ahead',\n",
    "      25:'Steep descent',\n",
    "      26:'Steep ascent',\n",
    "      27:'Road narrows on the right',\n",
    "      28:'Narrow bridge',\n",
    "      29:'Unprotected quay',\n",
    "      30:'Road hump',\n",
    "      31:'Bumpy road',\n",
    "      32:'Loose gravel',\n",
    "      33:'Falling rocks',\n",
    "      34:'Wild animals crossing',\n",
    "      35:'Crossroads',\n",
    "      36:'Side road junction',\n",
    "      37:'Oblique side road junction',\n",
    "      38:'T-junction',\n",
    "      39:'Y-junction',\n",
    "      40:'Staggered side road junction',\n",
    "      41:'Roundabout mandatory',\n",
    "      42:'Railroad head (with safety barriers)',\n",
    "      43:'Unguarded level crossing ahead',\n",
    "      44:'Parking',\n",
    "      45:'Bus stop'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge dataset (GTRSB + China + India)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data, merged_labels, merged_class_mapping = merge_datasets(merged_data, merged_labels, merged_class_mapping, india_train_data, india_train_label, india_classes)\n",
    "# Display the merged dataset shapes\n",
    "print(\"Merged Dataset Shapes:\")\n",
    "print(\"Merged Data Shape:\", merged_data.shape)\n",
    "print(\"Merged Labels Shape:\", merged_labels.shape)\n",
    "\n",
    "# Display the class mapping after merging\n",
    "print(\"Class Mapping After Merging:\")\n",
    "for key, value in merged_class_mapping.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "print(\"GTRSB Train data and label shape:\")\n",
    "print(GTRSB_data.shape, GTRSB_labels.shape) \n",
    "print(\"China Train data and label shape:\")\n",
    "print(china_train_data.shape, china_train_label.shape)\n",
    "print(\"India Train data and label shape:\")\n",
    "print(india_train_data.shape, india_train_label.shape)\n",
    "print(\"Merged Train shape:\")\n",
    "print(merged_data.shape, merged_labels.shape)\n",
    "print()\n",
    "test_labels = np.squeeze(test_labels)\n",
    "# Testing set\n",
    "print(\"GTRSB Test data and label shape:\")\n",
    "print(test_data.shape, test_labels.shape) \n",
    "print(\"China Test data and label shape:\")\n",
    "print(china_test_data.shape, china_test_label.shape)\n",
    "print(\"India Test data and label shape:\")\n",
    "print(india_test_data.shape, india_test_label.shape)\n",
    "# create an overall testset \n",
    "overall_test_data, overall_test_labels, overall_classes = merge_datasets(test_data, test_labels, GTSRB_classes, china_test_data, china_test_label, china_classes)\n",
    "overall_test_data, overall_test_labels, overall_classes = merge_datasets(overall_test_data, overall_test_labels, overall_classes, india_test_data, india_test_label, india_classes)\n",
    "\n",
    "print(\"Overall Test data and label shape:\")\n",
    "print(overall_test_data.shape, overall_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize images\n",
    "merged_data = merged_data / 255.0 # Train data\n",
    "overall_test_data = overall_test_data / 255.0\n",
    "test_data = test_data / 255.0 # GTRSB only test set\n",
    "china_test_data = china_test_data / 255.0\n",
    "india_test_data = india_test_data / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting labels into one hot encoding \n",
    "# train_labels = to_categorical(train_labels, 43)\n",
    "# test_labels = to_categorical(test_labels, 43)\n",
    "merged_labels = to_categorical(merged_labels, 106) # Train labels\n",
    "overall_test_labels = to_categorical(overall_test_labels, 106)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to be between 0 and 1\n",
    "# train_data = train_data / 255.0\n",
    "# will modify the splitting latter to create train and test set\n",
    "# Split the data into training and validation sets (80%, 20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(merged_data, merged_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "# model = models.Sequential([\n",
    "#     layers.InputLayer(input_shape=(30, 30, 3)), \n",
    "#     layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "#     layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "#     layers.Flatten(),\n",
    "#     layers.Dense(256, activation='relu'),\n",
    "#     layers.Dense(106, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# 2. \n",
    "# increased the model complexity by adding conv and dense layers, hoping for higher accuracy\n",
    "# added droupout layers to reduce overfitting\n",
    "# thoughts: since we merged datasets to increase the diversity of training samples, it can often allow \n",
    "# for an increase in model complexity without the immediate risk of overfitting by enabling it to learn\n",
    "# more robust and generalizable features.\n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=(30, 30, 3)),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    \n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),  # Adding dropout for regularization\n",
    "    \n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(106, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "model.save(\"traffic_sign_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting graphs for accuracy \n",
    "# https://github.com/deepak2233/Traffic-Signs-Recognition-using-CNN-Keras/blob/main/Model/Traffic%20Signs%20Recognition%20using%20CNN%20%26%20Keras%20with%2098%25%20Accuracy.ipynb\n",
    "plt.plot(history.history['accuracy'], label='training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Loss \n",
    "plt.plot(history.history['loss'], label='training loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall test using merged test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, num_classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    \n",
    "    classes = [str(i) for i in range(num_classes)]\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(num_classes, num_classes))\n",
    "    sns.set(font_scale=1.2)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, xticklabels=classes, yticklabels=classes)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_true, y_pred, zero_division=1))\n",
    "\n",
    "def analyze_confusion_matrix(y_true, y_pred, num_classes):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Identify high-error pairs\n",
    "    high_error_pairs = []\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            if i != j and cm[i, j] > 10:  # Adjust the threshold as needed\n",
    "                high_error_pairs.append((i, j, cm[i, j]))\n",
    "\n",
    "    # Print high-error pairs\n",
    "    if high_error_pairs:\n",
    "        print(\"High-Error Pairs:\")\n",
    "        for pair in high_error_pairs:\n",
    "            true_class, predicted_class, count = pair\n",
    "            print(f\"True Class {true_class} predicted as {predicted_class}: {count} instances\")\n",
    "    else:\n",
    "        print(\"No high-error pairs found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on overall test set\n",
    "model = models.load_model(\"traffic_sign_model\")\n",
    "y_pred = model.predict(overall_test_data)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(overall_test_labels, axis=1)\n",
    "print_metrics(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "plot_confusion_matrix(y_true, y_pred, 106, title='Overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_confusion_matrix(y_true, y_pred, 106)\n",
    "# results shows that class 33 (Turn right ahead) and 34 (Turn left ahead) are most commonly confused with each other\n",
    "# this could be due to the data argumentation of rotating images, causing it difficult to classified sometimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate using GTSRB only test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_labels(labels, current_class_mapping, target_class_mapping):\n",
    "    remapped_labels = []\n",
    "\n",
    "    for label in labels:\n",
    "        print(current_class_mapping)\n",
    "        original_value = current_class_mapping.get(label, None)\n",
    "        if original_value is not None:\n",
    "            original_key = next((key for key, value in target_class_mapping.items() if value.lower() == original_value.lower()), None)\n",
    "            if original_key is not None:\n",
    "                remapped_labels.append(original_key)\n",
    "            else:\n",
    "                remapped_labels.append(-1)\n",
    "        else:\n",
    "            remapped_labels.append(-1)\n",
    "\n",
    "    return remapped_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def map_output_to_target_classes(y_pred, target_classes_mapping):\n",
    "    # Create a reverse mapping for the target classes\n",
    "    reverse_target_mapping = {v: k for k, v in target_classes_mapping.items()}\n",
    "\n",
    "    # Map the output to target classes\n",
    "    y_pred_target_classes = [reverse_target_mapping[label] for label in y_pred]\n",
    "\n",
    "    return y_pred_target_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remap GTRSB test labels to match the 106 classes label format\n",
    "test_labels = remap_labels(test_labels, GTSRB_classes, merged_class_mapping)\n",
    "test_labels = to_categorical(test_labels, 106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on GTRSB only\n",
    "model = models.load_model(\"traffic_sign_model\")\n",
    "y_pred = model.predict(test_data)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(test_labels, axis=1)\n",
    "print(len(np.unique(y_true)))\n",
    "print_metrics(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *Have problem with printing confusion matrix correctly, when remapping the model output labels back to a smaller subset of labels such as GTRSB 43 classes label\n",
    "# remap to print confusion matrix\n",
    "y_pred_43 = remap_labels(y_pred, merged_class_mapping, GTSRB_classes)\n",
    "y_true_43 = remap_labels(y_true, merged_class_mapping, GTSRB_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "plot_confusion_matrix(y_true_43, y_pred_43, 44, title='GTRSB only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate using China only test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china_test_label = remap_labels(china_test_label, china_classes, merged_class_mapping)\n",
    "china_test_label = to_categorical(china_test_label, 106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(\"traffic_sign_model\")\n",
    "y_pred = model.predict(china_test_data)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(china_test_label, axis=1)\n",
    "print(len(np.unique(y_true)))\n",
    "print_metrics(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_58 = remap_labels(y_pred, merged_class_mapping, china_classes)\n",
    "y_true_58 = remap_labels(y_true, merged_class_mapping, china_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "plot_confusion_matrix(y_true_58, y_pred_58, 58, title='China Dataset only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate using only india dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_test_label = remap_labels(india_test_label, india_classes, merged_class_mapping)\n",
    "india_test_label = to_categorical(india_test_label, 106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(\"traffic_sign_model\")\n",
    "y_pred = model.predict(india_test_data)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(india_test_label, axis=1)\n",
    "print(len(np.unique(y_true)))\n",
    "print_metrics(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_45 = remap_labels(y_pred, merged_class_mapping, india_classes)\n",
    "y_true_45 = remap_labels(y_true, merged_class_mapping, india_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "plot_confusion_matrix(y_true_45, y_pred_45, 45, title='India Dataset only')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
